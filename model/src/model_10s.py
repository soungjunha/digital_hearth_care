import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import h5py
import matplotlib.pyplot as plt
from pathlib import Path
import warnings
from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve
import seaborn as sns
import platform
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
if platform.system() == 'Windows':
    plt.rc('font', family='Malgun Gothic')  # Windows
elif platform.system() == 'Darwin':
    plt.rc('font', family='AppleGothic')  # Mac
else:
    plt.rc('font', family='NanumGothic')  # Linux
plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€


# ===== HDF5 ë°ì´í„°ì…‹ í´ë˜ìŠ¤ =====
class HDF5CardiacDataset(Dataset):
    """HDF5 í¬ë§· ECG, PPG ë°ì´í„°ì…‹ (í‘œì¤€ êµ¬ì¡°)"""
    
    def __init__(self, hdf5_file, indices=None, load_to_memory=True):
        self.hdf5_file = hdf5_file
        self.load_to_memory = load_to_memory
        
        with h5py.File(hdf5_file, 'r') as hf:
            # HDF5 íŒŒì¼ êµ¬ì¡° í™•ì¸
            print(f"\nğŸ“‚ HDF5 íŒŒì¼ êµ¬ì¡° í™•ì¸: {Path(hdf5_file).name}")
            
            # êµ¬ì¡° íƒ€ì… ê°ì§€: ê·¸ë£¹ êµ¬ì¡° vs í‘œì¤€ êµ¬ì¡°
            keys = list(hf.keys())
            
            # ê·¸ë£¹ êµ¬ì¡°ì¸ì§€ í™•ì¸ (seg0000, seg0001 í˜•íƒœ)
            if keys and isinstance(hf[keys[0]], h5py.Group):
                print(f"   âš ï¸  ê·¸ë£¹ êµ¬ì¡° ê°ì§€! HDF5GroupCardiacDatasetì„ ì‚¬ìš©í•˜ì„¸ìš”.")
                raise ValueError("ì´ íŒŒì¼ì€ ê·¸ë£¹ êµ¬ì¡°ì…ë‹ˆë‹¤. HDF5GroupCardiacDatasetì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            
            print(f"   ë°ì´í„°ì…‹ í‚¤: {keys}")
            if hf.attrs:
                print(f"   Attributes: {dict(hf.attrs)}")
            
            # ë°ì´í„°ì…‹ í‚¤ ìë™ ê°ì§€
            if 'ecg' in hf.keys() and 'ppg' in hf.keys():
                ecg_key, ppg_key = 'ecg', 'ppg'
            elif 'ECG' in hf.keys() and 'PPG' in hf.keys():
                ecg_key, ppg_key = 'ECG', 'PPG'
            else:
                raise KeyError(f"ECG/PPG ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤: {keys}")
            
            # attributesê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë°ì´í„° shapeì—ì„œ ì¶”ì¶œ
            if 'n_samples' in hf.attrs:
                self.n_samples = hf.attrs['n_samples']
                self.sequence_length = hf.attrs['sequence_length']
                self.sampling_rate = hf.attrs.get('sampling_rate', 256)
            else:
                self.n_samples = hf[ecg_key].shape[0]
                self.sequence_length = hf[ecg_key].shape[1]
                self.sampling_rate = 256
                print(f"   âš ï¸  Attributes ì—†ìŒ. ìë™ ì¶”ì¶œ: n_samples={self.n_samples}, seq_len={self.sequence_length}")
            
            if indices is None:
                self.indices = list(range(self.n_samples))
            else:
                self.indices = indices
            
            if load_to_memory:
                self.ecg_data = hf[ecg_key][:].astype(np.float32)
                self.ppg_data = hf[ppg_key][:].astype(np.float32)
                print(f"   âœ… ë©”ëª¨ë¦¬ ë¡œë“œ ì™„ë£Œ: ECG={self.ecg_data.shape}, PPG={self.ppg_data.shape}")
            else:
                self.ecg_data = None
                self.ppg_data = None
                self.ecg_key = ecg_key
                self.ppg_key = ppg_key
    
    def __len__(self):
        return len(self.indices)
    
    def __getitem__(self, idx):
        real_idx = self.indices[idx]
        
        if self.load_to_memory:
            ecg = self.ecg_data[real_idx]
            ppg = self.ppg_data[real_idx]
        else:
            with h5py.File(self.hdf5_file, 'r') as hf:
                ecg_key = getattr(self, 'ecg_key', 'ecg')
                ppg_key = getattr(self, 'ppg_key', 'ppg')
                ecg = hf[ecg_key][real_idx]
                ppg = hf[ppg_key][real_idx]
        
        sample = np.stack([ecg, ppg], axis=0)
        sample = torch.FloatTensor(sample)
        
        return sample, sample


class HDF5GroupCardiacDataset(Dataset):
    """ê·¸ë£¹ êµ¬ì¡°ì˜ HDF5 í¬ë§· ECG, PPG ë°ì´í„°ì…‹"""
    
    def __init__(self, hdf5_file, indices=None, load_to_memory=True):
        self.hdf5_file = hdf5_file
        self.load_to_memory = load_to_memory
        
        with h5py.File(hdf5_file, 'r') as hf:
            print(f"\nğŸ“‚ ê·¸ë£¹ êµ¬ì¡° HDF5 íŒŒì¼: {Path(hdf5_file).name}")
            
            # ëª¨ë“  ê·¸ë£¹ ì´ë¦„ ìˆ˜ì§‘
            self.group_names = sorted([key for key in hf.keys() if isinstance(hf[key], h5py.Group)])
            self.n_samples = len(self.group_names)
            
            # ì²« ë²ˆì§¸ ê·¸ë£¹ì—ì„œ sequence_length í™•ì¸
            if self.n_samples > 0:
                first_group = hf[self.group_names[0]]
                self.sequence_length = first_group['ecg'].shape[0]
                self.sampling_rate = 256
            else:
                raise ValueError("ê·¸ë£¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            print(f"   ê·¸ë£¹ ê°œìˆ˜: {self.n_samples}")
            print(f"   ì‹œí€€ìŠ¤ ê¸¸ì´: {self.sequence_length}")
            print(f"   ì²« ë²ˆì§¸ ê·¸ë£¹: {self.group_names[0]}")
            print(f"   ë§ˆì§€ë§‰ ê·¸ë£¹: {self.group_names[-1]}")
            
            if indices is None:
                self.indices = list(range(self.n_samples))
            else:
                self.indices = indices
            
            # ë©”ëª¨ë¦¬ì— ë¡œë“œ
            if load_to_memory:
                ecg_list = []
                ppg_list = []
                
                print(f"   ë©”ëª¨ë¦¬ ë¡œë“œ ì¤‘...")
                for group_name in self.group_names:
                    group = hf[group_name]
                    ecg = group['ecg'][:].astype(np.float32)
                    ppg = group['ppg'][:].astype(np.float32)
                    ecg_list.append(ecg)
                    ppg_list.append(ppg)
                
                self.ecg_data = np.array(ecg_list, dtype=np.float32)
                self.ppg_data = np.array(ppg_list, dtype=np.float32)
                print(f"   âœ… ë©”ëª¨ë¦¬ ë¡œë“œ ì™„ë£Œ: ECG={self.ecg_data.shape}, PPG={self.ppg_data.shape}")
            else:
                self.ecg_data = None
                self.ppg_data = None
    
    def __len__(self):
        return len(self.indices)
    
    def __getitem__(self, idx):
        real_idx = self.indices[idx]
        
        if self.load_to_memory:
            ecg = self.ecg_data[real_idx]
            ppg = self.ppg_data[real_idx]
        else:
            with h5py.File(self.hdf5_file, 'r') as hf:
                group_name = self.group_names[real_idx]
                ecg = hf[group_name]['ecg'][:].astype(np.float32)
                ppg = hf[group_name]['ppg'][:].astype(np.float32)
        
        sample = np.stack([ecg, ppg], axis=0)
        sample = torch.FloatTensor(sample)
        
        return sample, sample


def load_dataset_auto(hdf5_file, indices=None, load_to_memory=True):
    """HDF5 êµ¬ì¡°ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•˜ì—¬ ì ì ˆí•œ Dataset ë°˜í™˜"""
    with h5py.File(hdf5_file, 'r') as hf:
        keys = list(hf.keys())
        
        # ê·¸ë£¹ êµ¬ì¡°ì¸ì§€ í™•ì¸
        if keys and isinstance(hf[keys[0]], h5py.Group):
            return HDF5GroupCardiacDataset(hdf5_file, indices, load_to_memory)
        else:
            return HDF5CardiacDataset(hdf5_file, indices, load_to_memory)


def create_train_val_test_datasets(hdf5_file, train_ratio=0.75, val_ratio=0.15, load_to_memory=True):
    """í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì‚¼ë¶„í•  (7.5:1.5:1)"""
    with h5py.File(hdf5_file, 'r') as hf:
        # í‚¤ ìë™ ê°ì§€
        if 'ecg' in hf.keys():
            ecg_key = 'ecg'
        elif 'ECG' in hf.keys():
            ecg_key = 'ECG'
        else:
            # ê·¸ë£¹ êµ¬ì¡°ì¸ ê²½ìš°
            keys = list(hf.keys())
            if keys and isinstance(hf[keys[0]], h5py.Group):
                n_samples = len([k for k in keys if isinstance(hf[k], h5py.Group)])
            else:
                raise KeyError(f"ECG ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í‚¤: {list(hf.keys())}")
        
        if 'n_samples' in hf.attrs:
            n_samples = hf.attrs['n_samples']
        else:
            if 'ecg' in hf.keys():
                n_samples = hf['ecg'].shape[0]
            elif 'ECG' in hf.keys():
                n_samples = hf['ECG'].shape[0]
    
    indices = np.arange(n_samples)
    np.random.shuffle(indices)
    
    train_end = int(len(indices) * train_ratio)
    val_end = train_end + int(len(indices) * val_ratio)
    
    train_indices = indices[:train_end].tolist()
    val_indices = indices[train_end:val_end].tolist()
    test_indices = indices[val_end:].tolist()
    
    print(f"\nğŸ“¦ ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
    print(f"   ì „ì²´: {n_samples:,}ê°œ")
    print(f"   í•™ìŠµ: {len(train_indices):,}ê°œ ({len(train_indices)/n_samples*100:.1f}%)")
    print(f"   ê²€ì¦: {len(val_indices):,}ê°œ ({len(val_indices)/n_samples*100:.1f}%)")
    print(f"   í…ŒìŠ¤íŠ¸: {len(test_indices):,}ê°œ ({len(test_indices)/n_samples*100:.1f}%)")
    
    train_dataset = load_dataset_auto(hdf5_file, train_indices, load_to_memory)
    val_dataset = load_dataset_auto(hdf5_file, val_indices, load_to_memory)
    test_dataset = load_dataset_auto(hdf5_file, test_indices, load_to_memory)
    
    return train_dataset, val_dataset, test_dataset


# ===== CNN-GRU Autoencoder ëª¨ë¸ =====
class CNNGRUAutoencoder(nn.Module):
    def __init__(self, input_channels=2, sequence_length=2560, latent_dim=128):
        super().__init__()
        
        # Encoder: CNN + GRU
        self.encoder_cnn = nn.Sequential(
            nn.Conv1d(input_channels, 32, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            
            nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=2),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            
            nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm1d(128),
            nn.ReLU()
        )
        
        self.cnn_output_length = sequence_length // 8
        self.encoder_gru = nn.GRU(128, latent_dim, num_layers=2, batch_first=True, bidirectional=False)
        
        # Decoder: GRU + CNN
        self.decoder_gru = nn.GRU(latent_dim, 128, num_layers=2, batch_first=True, bidirectional=False)
        
        self.decoder_cnn = nn.Sequential(
            nn.ConvTranspose1d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            
            nn.ConvTranspose1d(64, 32, kernel_size=5, stride=2, padding=2, output_padding=1),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            
            nn.ConvTranspose1d(32, input_channels, kernel_size=7, stride=2, padding=3, output_padding=1),
            nn.Tanh()
        )
    
    def forward(self, x):
        # Encoder
        x_cnn = self.encoder_cnn(x)
        x_cnn = x_cnn.permute(0, 2, 1)
        _, hidden = self.encoder_gru(x_cnn)
        
        # Decoder
        latent = hidden[-1].unsqueeze(1).repeat(1, self.cnn_output_length, 1)
        x_gru, _ = self.decoder_gru(latent)
        x_gru = x_gru.permute(0, 2, 1)
        reconstructed = self.decoder_cnn(x_gru)
        
        return reconstructed, hidden


# ===== ì„±ëŠ¥ í‰ê°€ í´ë˜ìŠ¤ =====
class MedicalDevicePerformanceEvaluator:
    """ì‹ì•½ì²˜ ì²´ì™¸ì§„ë‹¨ì˜ë£Œê¸°ê¸° ê°€ì´ë“œë¼ì¸ ê¸°ë°˜ ì„±ëŠ¥ í‰ê°€"""
    
    def __init__(self, model, threshold_percentile=95):
        self.model = model
        self.threshold_percentile = threshold_percentile
        self.threshold = None
    
    def fit_threshold(self, train_loader, device='cpu'):
        """ì •ìƒ ë°ì´í„° í•™ìŠµ ì„¸íŠ¸ë¡œ ì„ê³„ê°’ ì„¤ì •"""
        self.model.eval()
        errors = []
        
        with torch.no_grad():
            for data, target in train_loader:
                data, target = data.to(device), target.to(device)
                reconstructed, _ = self.model(data)
                error = torch.mean((reconstructed - target) ** 2, dim=(1, 2)).cpu().numpy()
                errors.extend(error)
                
                del data, target, reconstructed, error
                if device.type == 'cuda':
                    torch.cuda.empty_cache()
        
        self.threshold = np.percentile(errors, self.threshold_percentile)
        print(f"   ì„ê³„ê°’ (Percentile {self.threshold_percentile}): {self.threshold:.6f}")
        print(f"   í•™ìŠµ ë°ì´í„° ì˜¤ì°¨ ë²”ìœ„: [{np.min(errors):.6f}, {np.max(errors):.6f}]")
    
    def evaluate_dataset(self, data_loader, true_label, dataset_name, device='cpu'):
        """ë°ì´í„°ì…‹ í‰ê°€ (true_label: 0=ì •ìƒ, 1=ë¹„ì •ìƒ)"""
        self.model.eval()
        errors = []
        labels = []
        predictions = []
        
        with torch.no_grad():
            for data, target in data_loader:
                data, target = data.to(device), target.to(device)
                reconstructed, _ = self.model(data)
                error = torch.mean((reconstructed - target) ** 2, dim=(1, 2)).cpu().numpy()
                errors.extend(error)
                labels.extend([true_label] * len(error))
                
                del data, target, reconstructed
                if device.type == 'cuda':
                    torch.cuda.empty_cache()
        
        errors = np.array(errors)
        predictions = (errors > self.threshold).astype(int)
        
        print(f"   {dataset_name} ì˜¤ì°¨ ë²”ìœ„: [{errors.min():.6f}, {errors.max():.6f}]")
        print(f"   {dataset_name} ì˜ˆì¸¡: ì •ìƒ {np.sum(predictions==0)}, ë¹„ì •ìƒ {np.sum(predictions==1)}")
        
        return errors, np.array(labels), predictions
    
    def calculate_medical_metrics(self, labels, predictions, errors):
        """ì‹ì•½ì²˜ ê°€ì´ë“œë¼ì¸ ê¸°ë°˜ ì„±ëŠ¥ ì§€í‘œ"""
        tn, fp, fn, tp = confusion_matrix(labels, predictions).ravel()
        
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0
        npv = tn / (tn + fn) if (tn + fn) > 0 else 0
        accuracy = (tp + tn) / (tp + tn + fp + fn)
        f1 = 2 * (ppv * sensitivity) / (ppv + sensitivity) if (ppv + sensitivity) > 0 else 0
        
        fpr, tpr, _ = roc_curve(labels, errors)
        roc_auc = auc(fpr, tpr)
        
        precision, recall, _ = precision_recall_curve(labels, errors)
        pr_auc = auc(recall, precision)
        
        return {
            'TP': int(tp), 'TN': int(tn), 'FP': int(fp), 'FN': int(fn),
            'Sensitivity': sensitivity,
            'Specificity': specificity,
            'PPV': ppv,
            'NPV': npv,
            'Accuracy': accuracy,
            'F1_Score': f1,
            'ROC_AUC': roc_auc,
            'PR_AUC': pr_auc,
            'Threshold': self.threshold,
            'FPR': fpr,
            'TPR': tpr,
            'Precision_curve': precision,
            'Recall_curve': recall
        }


def plot_medical_performance_dashboard(metrics, normal_errors, abnormal_errors, save_path=None):
    """ì‹ì•½ì²˜ ê°€ì´ë“œë¼ì¸ ê¸°ë°˜ ì‹œê°í™” ëŒ€ì‹œë³´ë“œ"""
    
    # ëŒ€ì‹œë³´ë“œ ìœˆë„ìš°
    fig = plt.figure(figsize=(18, 12))
    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
    
    # 1. Confusion Matrix
    ax1 = fig.add_subplot(gs[0, 0])
    cm = np.array([[metrics['TN'], metrics['FP']], [metrics['FN'], metrics['TP']]])
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax1,
                xticklabels=['ì˜ˆì¸¡: ì •ìƒ', 'ì˜ˆì¸¡: ë¹„ì •ìƒ'],
                yticklabels=['ì‹¤ì œ: ì •ìƒ', 'ì‹¤ì œ: ë¹„ì •ìƒ'])
    ax1.set_title('Confusion Matrix', fontsize=14, fontweight='bold')
    
    # 2. ì£¼ìš” ì§€í‘œ í…ìŠ¤íŠ¸
    ax2 = fig.add_subplot(gs[0, 1])
    ax2.axis('off')
    metrics_text = f"""
    ë¯¼ê°ë„: {metrics['Sensitivity']:.4f} ({metrics['Sensitivity']*100:.2f}%)
    íŠ¹ì´ë„: {metrics['Specificity']:.4f} ({metrics['Specificity']*100:.2f}%)
    PPV:    {metrics['PPV']:.4f} ({metrics['PPV']*100:.2f}%)
    NPV:    {metrics['NPV']:.4f} ({metrics['NPV']*100:.2f}%)
    ì •í™•ë„: {metrics['Accuracy']:.4f} ({metrics['Accuracy']*100:.2f}%)
    F1:     {metrics['F1_Score']:.4f}
    
    ROC-AUC: {metrics['ROC_AUC']:.4f}
    PR-AUC:  {metrics['PR_AUC']:.4f}
    """
    ax2.text(0.1, 0.5, metrics_text, fontsize=13, verticalalignment='center',
             family='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))
    ax2.set_title('ì‹ì•½ì²˜ ê°€ì´ë“œë¼ì¸ ì§€í‘œ', fontsize=14, fontweight='bold')
    
    # 3. ì¬êµ¬ì„± ì˜¤ì°¨ ë¶„í¬
    ax3 = fig.add_subplot(gs[0, 2])
    ax3.hist(normal_errors, bins=50, alpha=0.6, color='#3498DB', edgecolor='black',
             label=f'ì •ìƒ (n={len(normal_errors)})', density=True)
    ax3.hist(abnormal_errors, bins=50, alpha=0.6, color='#E74C3C', edgecolor='black',
             label=f'ë¹„ì •ìƒ (n={len(abnormal_errors)})', density=True)
    ax3.axvline(metrics['Threshold'], color='green', linestyle='--', linewidth=3,
                label=f"ì„ê³„ê°’ = {metrics['Threshold']:.6f}")
    ax3.set_xlabel('ì¬êµ¬ì„± ì˜¤ì°¨ (MSE)', fontsize=11)
    ax3.set_ylabel('ë°€ë„', fontsize=11)
    ax3.set_title('ì •ìƒ vs ë¹„ì •ìƒ ì˜¤ì°¨ ë¶„í¬', fontsize=14, fontweight='bold')
    ax3.legend(fontsize=9)
    ax3.grid(True, alpha=0.3, axis='y')
    
    # 4. ROC Curve
    ax4 = fig.add_subplot(gs[1, 0])
    ax4.plot(metrics['FPR'], metrics['TPR'], color='#E74C3C', linewidth=2.5,
             label=f'ROC (AUC = {metrics["ROC_AUC"]:.3f})')
    ax4.plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Random Classifier')
    ax4.fill_between(metrics['FPR'], metrics['TPR'], alpha=0.3, color='#E74C3C')
    ax4.set_xlabel('False Positive Rate', fontsize=11)
    ax4.set_ylabel('True Positive Rate', fontsize=11)
    ax4.set_title('ROC Curve', fontsize=14, fontweight='bold')
    ax4.legend(loc='lower right', fontsize=10)
    ax4.grid(True, alpha=0.3)
    
    # 5. Precision-Recall Curve
    ax5 = fig.add_subplot(gs[1, 1])
    ax5.plot(metrics['Recall_curve'], metrics['Precision_curve'],
             color='#9B59B6', linewidth=2.5, label=f'PR (AUC = {metrics["PR_AUC"]:.3f})')
    ax5.fill_between(metrics['Recall_curve'], metrics['Precision_curve'], alpha=0.3, color='#9B59B6')
    ax5.set_xlabel('Recall', fontsize=11)
    ax5.set_ylabel('Precision', fontsize=11)
    ax5.set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')
    ax5.legend(loc='upper right', fontsize=10)
    ax5.grid(True, alpha=0.3)
    
    # 6. ë°”ì´ì˜¬ë¦° í”Œë¡¯
    ax6 = fig.add_subplot(gs[1, 2])
    data_violin = [normal_errors, abnormal_errors]
    parts = ax6.violinplot(data_violin, positions=[1, 2], showmeans=True, showmedians=True)
    for pc in parts['bodies']:
        pc.set_facecolor('#3498DB')
        pc.set_alpha(0.6)
    ax6.set_xticks([1, 2])
    ax6.set_xticklabels(['ì •ìƒ', 'ë¹„ì •ìƒ'])
    ax6.set_ylabel('ì¬êµ¬ì„± ì˜¤ì°¨ (MSE)', fontsize=11)
    ax6.set_title('ì˜¤ì°¨ ë¶„í¬ (Violin Plot)', fontsize=14, fontweight='bold')
    ax6.axhline(metrics['Threshold'], color='green', linestyle='--', linewidth=2, label='ì„ê³„ê°’')
    ax6.legend(fontsize=9)
    ax6.grid(True, alpha=0.3, axis='y')
    
    # 7. ë°•ìŠ¤í”Œë¡¯
    ax7 = fig.add_subplot(gs[2, 0])
    bp = ax7.boxplot(data_violin, labels=['ì •ìƒ', 'ë¹„ì •ìƒ'], patch_artist=True,
                     boxprops=dict(facecolor='lightblue', color='black'),
                     medianprops=dict(color='red', linewidth=2))
    ax7.set_ylabel('ì¬êµ¬ì„± ì˜¤ì°¨ (MSE)', fontsize=11)
    ax7.set_title('ì˜¤ì°¨ ë¶„í¬ (Box Plot)', fontsize=14, fontweight='bold')
    ax7.axhline(metrics['Threshold'], color='green', linestyle='--', linewidth=2, label='ì„ê³„ê°’')
    ax7.legend(fontsize=9)
    ax7.grid(True, alpha=0.3, axis='y')
    
    # 8. í†µê³„ ìš”ì•½
    ax8 = fig.add_subplot(gs[2, 1])
    ax8.axis('off')
    stats_text = f"""
    ì •ìƒ ë°ì´í„°:
      í‰ê· : {np.mean(normal_errors):.6f}
      ì¤‘ì•™ê°’: {np.median(normal_errors):.6f}
      í‘œì¤€í¸ì°¨: {np.std(normal_errors):.6f}
      ë²”ìœ„: [{np.min(normal_errors):.6f}, {np.max(normal_errors):.6f}]
    
    ë¹„ì •ìƒ ë°ì´í„°:
      í‰ê· : {np.mean(abnormal_errors):.6f}
      ì¤‘ì•™ê°’: {np.median(abnormal_errors):.6f}
      í‘œì¤€í¸ì°¨: {np.std(abnormal_errors):.6f}
      ë²”ìœ„: [{np.min(abnormal_errors):.6f}, {np.max(abnormal_errors):.6f}]
    """
    ax8.text(0.1, 0.5, stats_text, fontsize=10, verticalalignment='center',
             family='monospace', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.3))
    ax8.set_title('í†µê³„ ìš”ì•½', fontsize=14, fontweight='bold')
    
    # 9. ì„±ëŠ¥ ì§€í‘œ ë§‰ëŒ€ ê·¸ë˜í”„
    ax9 = fig.add_subplot(gs[2, 2])
    metrics_names = ['ë¯¼ê°ë„', 'íŠ¹ì´ë„', 'PPV', 'NPV', 'ì •í™•ë„', 'F1']
    metrics_values = [metrics['Sensitivity'], metrics['Specificity'],
                      metrics['PPV'], metrics['NPV'], metrics['Accuracy'], metrics['F1_Score']]
    colors = ['#3498DB', '#E74C3C', '#2ECC71', '#F39C12', '#9B59B6', '#1ABC9C']
    bars = ax9.barh(metrics_names, metrics_values, color=colors, edgecolor='black')
    ax9.set_xlim(0, 1)
    ax9.set_xlabel('ê°’', fontsize=11)
    ax9.set_title('ì„±ëŠ¥ ì§€í‘œ ìš”ì•½', fontsize=14, fontweight='bold')
    ax9.grid(True, alpha=0.3, axis='x')
    for i, (bar, val) in enumerate(zip(bars, metrics_values)):
        ax9.text(val + 0.02, i, f'{val:.3f}', va='center', fontsize=10)
    
    plt.suptitle('ì‹ì•½ì²˜ ì²´ì™¸ì§„ë‹¨ì˜ë£Œê¸°ê¸° ì„±ëŠ¥ í‰ê°€ ëŒ€ì‹œë³´ë“œ', fontsize=18, fontweight='bold', y=0.98)
    
    if save_path:
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\nğŸ“Š ëŒ€ì‹œë³´ë“œ ì €ì¥: {save_path}")
    
    # --- [ì¶”ê°€ëœ ë¶€ë¶„: 5ê°œ ê°œë³„ ìœˆë„ìš° ìƒì„±] ---
    try:
        print("\n--- ê°œë³„ ìœˆë„ìš° ìƒì„± ì‹œì‘ ---")
        
        # 1. Confusion Matrix (ìƒˆ ìœˆë„ìš°)
        plt.figure(figsize=(8, 6))
        cm = np.array([[metrics['TN'], metrics['FP']], [metrics['FN'], metrics['TP']]])
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,
                    xticklabels=['ì˜ˆì¸¡: ì •ìƒ', 'ì˜ˆì¸¡: ë¹„ì •ìƒ'],
                    yticklabels=['ì‹¤ì œ: ì •ìƒ', 'ì‹¤ì œ: ë¹„ì •ìƒ'],
                    annot_kws={"size": 16})
        plt.title('Confusion Matrix (ê°œë³„ ìœˆë„ìš°)', fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        # 2. ì£¼ìš” ì§€í‘œ (ìƒˆ ìœˆë„ìš°)
        fig2 = plt.figure(figsize=(10, 8))
        ax = fig2.add_subplot(111)
        ax.axis('off')
        metrics_text_large = f"""
        ğŸ“Š ì‹ì•½ì²˜ ê°€ì´ë“œë¼ì¸ ì„±ëŠ¥ ì§€í‘œ
        
        ë¯¼ê°ë„ (Sensitivity):  {metrics['Sensitivity']:.4f} ({metrics['Sensitivity']*100:.2f}%)
        íŠ¹ì´ë„ (Specificity):  {metrics['Specificity']:.4f} ({metrics['Specificity']*100:.2f}%)
        
        PPV (ì–‘ì„± ì˜ˆì¸¡ë„):     {metrics['PPV']:.4f} ({metrics['PPV']*100:.2f}%)
        NPV (ìŒì„± ì˜ˆì¸¡ë„):     {metrics['NPV']:.4f} ({metrics['NPV']*100:.2f}%)
        
        ì •í™•ë„ (Accuracy):     {metrics['Accuracy']:.4f} ({metrics['Accuracy']*100:.2f}%)
        F1 Score:              {metrics['F1_Score']:.4f}
        
        ROC-AUC:               {metrics['ROC_AUC']:.4f}
        PR-AUC:                {metrics['PR_AUC']:.4f}
        
        ì„ê³„ê°’ (Threshold):    {metrics['Threshold']:.6f}
        """
        ax.text(0.5, 0.5, metrics_text_large, fontsize=16, verticalalignment='center',
                horizontalalignment='center', family='monospace',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5, pad=1))
        
        # 3. ROC Curve (ìƒˆ ìœˆë„ìš°)
        plt.figure(figsize=(10, 8))
        plt.plot(metrics['FPR'], metrics['TPR'], color='#E74C3C', linewidth=3,
                 label=f'ROC (AUC = {metrics["ROC_AUC"]:.3f})')
        plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')
        plt.fill_between(metrics['FPR'], metrics['TPR'], alpha=0.3, color='#E74C3C')
        plt.xlabel('False Positive Rate (1 - íŠ¹ì´ë„)', fontsize=12)
        plt.ylabel('True Positive Rate (ë¯¼ê°ë„)', fontsize=12)
        plt.title('ROC Curve (ê°œë³„ ìœˆë„ìš°)', fontsize=14, fontweight='bold')
        plt.legend(loc='lower right', fontsize=11)
        plt.grid(True, alpha=0.3)
        
        # 4. Precision-Recall Curve (ìƒˆ ìœˆë„ìš°)
        plt.figure(figsize=(10, 8))
        plt.plot(metrics['Recall_curve'], metrics['Precision_curve'],
                 color='#9B59B6', linewidth=2.5, label=f'PR (AUC = {metrics["PR_AUC"]:.3f})')
        plt.fill_between(metrics['Recall_curve'], metrics['Precision_curve'], alpha=0.3, color='#9B59B6')
        plt.xlabel('Recall', fontsize=12)
        plt.ylabel('Precision', fontsize=12)
        plt.title('Precision-Recall Curve (ê°œë³„ ìœˆë„ìš°)', fontsize=14, fontweight='bold')
        plt.legend(loc='upper right')
        plt.grid(True, alpha=0.3)
        
        # 5. ì¬êµ¬ì„± ì˜¤ì°¨ ë¶„í¬ (ìƒˆ ìœˆë„ìš°)
        plt.figure(figsize=(10, 6))
        plt.hist(normal_errors, bins=50, alpha=0.6, color='#3498DB', edgecolor='black',
                 label=f'ì •ìƒ (n={len(normal_errors)})', density=True)
        plt.hist(abnormal_errors, bins=50, alpha=0.6, color='#E74C3C', edgecolor='black',
                 label=f'ë¹„ì •ìƒ (n={len(abnormal_errors)})', density=True)
        plt.axvline(metrics['Threshold'], color='green', linestyle='--', linewidth=3,
                    label=f"ì„ê³„ê°’ = {metrics['Threshold']:.6f}")
        plt.xlabel('ì¬êµ¬ì„± ì˜¤ì°¨ (MSE)', fontsize=12)
        plt.ylabel('ë°€ë„', fontsize=12)
        plt.title('ì •ìƒ vs ë¹„ì •ìƒ ì¬êµ¬ì„± ì˜¤ì°¨ ë¶„í¬ (ê°œë³„ ìœˆë„ìš°)', fontsize=14, fontweight='bold')
        plt.legend(fontsize=11)
        plt.grid(True, alpha=0.3, axis='y')
        
        print("--- âœ… ê°œë³„ ìœˆë„ìš° 5ê°œ ìƒì„± ì™„ë£Œ ---")
        
    except Exception as e:
        print(f"--- âš ï¸ ê°œë³„ ìœˆë„ìš° ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} ---")
    # --- [ì¶”ê°€ëœ ë¶€ë¶„ ë] ---

    # ê¸°ì¡´ ëŒ€ì‹œë³´ë“œ ìœˆë„ìš° (1ê°œ) + ê°œë³„ ìœˆë„ìš° (5ê°œ) = ì´ 6ê°œ ìœˆë„ìš°ê°€ ëœ¸
    plt.show()


def print_performance_report(metrics):
    """ì„±ëŠ¥ í‰ê°€ ë¦¬í¬íŠ¸"""
    print("\n" + "="*80)
    print("ğŸ“‹ ì‹ì•½ì²˜ ì²´ì™¸ì§„ë‹¨ì˜ë£Œê¸°ê¸° ì„±ëŠ¥ í‰ê°€ ë¦¬í¬íŠ¸")
    print("="*80)
    print(f"\n[Confusion Matrix]")
    print(f"  TP: {metrics['TP']:5d}  |  TN: {metrics['TN']:5d}  |  FP: {metrics['FP']:5d}  |  FN: {metrics['FN']:5d}")
    print(f"\n[ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ]")
    print(f"  ë¯¼ê°ë„: {metrics['Sensitivity']:.4f} ({metrics['Sensitivity']*100:.2f}%)")
    print(f"  íŠ¹ì´ë„: {metrics['Specificity']:.4f} ({metrics['Specificity']*100:.2f}%)")
    print(f"  PPV:    {metrics['PPV']:.4f} ({metrics['PPV']*100:.2f}%)")
    print(f"  NPV:    {metrics['NPV']:.4f} ({metrics['NPV']*100:.2f}%)")
    print(f"  ì •í™•ë„: {metrics['Accuracy']:.4f} ({metrics['Accuracy']*100:.2f}%)")
    print(f"  F1:     {metrics['F1_Score']:.4f}")
    print(f"\n[AUC]")
    print(f"  ROC: {metrics['ROC_AUC']:.4f}  |  PR: {metrics['PR_AUC']:.4f}")
    print("="*80 + "\n")


def train_autoencoder(model, train_loader, val_loader, epochs=100, lr=0.001, device='cpu', model_save_path='best_model.pth'):
    """ì˜¤í† ì¸ì½”ë” í•™ìŠµ"""
    from tqdm import tqdm
    
    Path(model_save_path).parent.mkdir(parents=True, exist_ok=True)
    
    model = model.to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)
    
    best_val_loss = float('inf')
    
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        
        for data, target in tqdm(train_loader, desc=f'[Epoch {epoch+1}/{epochs}] Training'):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            reconstructed, _ = model(data)
            loss = criterion(reconstructed, target)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            del data, target, reconstructed, loss
            if device.type == 'cuda':
                torch.cuda.empty_cache()
        
        train_loss /= len(train_loader)
        
        model.eval()
        val_loss = 0
        
        with torch.no_grad():
            for data, target in tqdm(val_loader, desc=f'[Epoch {epoch+1}/{epochs}] Validation'):
                data, target = data.to(device), target.to(device)
                reconstructed, _ = model(data)
                loss = criterion(reconstructed, target)
                val_loss += loss.item()
                del data, target, reconstructed, loss
                if device.type == 'cuda':
                    torch.cuda.empty_cache()
        
        val_loss /= len(val_loader)
        scheduler.step(val_loss)
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), model_save_path)
            print(f'Epoch [{epoch+1}/{epochs}], Train: {train_loss:.6f}, Val: {val_loss:.6f} â­')
        else:
            print(f'Epoch [{epoch+1}/{epochs}], Train: {train_loss:.6f}, Val: {val_loss:.6f}')


# ===== ë©”ì¸ =====
if __name__ == "__main__":
    # íŒŒì¼ ê²½ë¡œ (ì‚¬ìš©ìê°€ ìˆ˜ì •)
    NORMAL_HDF5_FILE = r'F:\codingìë£Œ\coding\digital_hearth_care\model_2\dataset_10sec.h5'
    
    # ===== ë¹„ì •ìƒ ë°ì´í„°ì…‹ 5ê°œ =====
    ABNORMAL_HDF5_FILES = [
        r'F:\codingìë£Œ\coding\digital_hearth_care\model_2\AF_10s.h5',
        r'F:\codingìë£Œ\coding\digital_hearth_care\model_2\Arrhythmia_10s.h5',
        r'F:\codingìë£Œ\coding\digital_hearth_care\model_2\HF_10s.h5',
        r'F:\codingìë£Œ\coding\digital_hearth_care\model_2\hypertension_10s.h5',
        r'F:\codingìë£Œ\coding\digital_hearth_care\model_2\IHD_10s.h5'
    ]
    
    MODEL_SAVE_PATH = r'F:\codingìë£Œ\coding\digital_hearth_care\model_2\10sec\model_test.pth'
    OUTPUT_DIR = r'F:\codingìë£Œ\coding\digital_hearth_care\model_2\10sec\performance'
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„°
    BATCH_SIZE = 128
    EVAL_BATCH_SIZE = 64
    EPOCHS = 100
    LEARNING_RATE = 0.001
    LATENT_DIM = 128
    LOAD_TO_MEMORY = True
    THRESHOLD_PERCENTILE = 95
    
    # ë°ì´í„° ë¶„í•  ë¹„ìœ¨ (7.5:1.5:1)
    TRAIN_RATIO = 0.75  # 75%
    VAL_RATIO = 0.15    # 15%
    TEST_RATIO = 0.10   # 10%
    
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print("="*80)
    print("ğŸ¥ ì‹¬ì¥ ì´ìƒì§•í›„ íƒì§€ - ì‹ì•½ì²˜ ê°€ì´ë“œë¼ì¸ ê¸°ë°˜ ì„±ëŠ¥ í‰ê°€ (ë¹„ì •ìƒ ë°ì´í„°ì…‹ 5ê°œ)")
    print("="*80)
    print(f"ğŸ“ ì •ìƒ ë°ì´í„°: {NORMAL_HDF5_FILE}")
    print(f"ğŸ“ ë¹„ì •ìƒ ë°ì´í„°ì…‹ ê°œìˆ˜: {len(ABNORMAL_HDF5_FILES)}ê°œ")
    for i, filepath in enumerate(ABNORMAL_HDF5_FILES, 1):
        print(f"   {i}. {filepath}")
    print(f"ğŸ–¥ï¸  ë””ë°”ì´ìŠ¤: {DEVICE}")
    print("="*80)
    
    # ë¹„ì •ìƒ íŒŒì¼ ì¡´ì¬ í™•ì¸
    missing_files = []
    for filepath in ABNORMAL_HDF5_FILES:
        if not Path(filepath).exists():
            missing_files.append(filepath)
    
    if missing_files:
        print(f"\nâŒ ë‹¤ìŒ ë¹„ì •ìƒ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤:")
        for filepath in missing_files:
            print(f"   - {filepath}")
        print(f"\n   ë¹„ì •ìƒ ë°ì´í„° HDF5 íŒŒì¼ì„ ì¤€ë¹„í•´ì£¼ì„¸ìš”.")
        exit(1)
    
    # í•™ìŠµ (ë˜ëŠ” ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ)
    print("\n[1ë‹¨ê³„] ëª¨ë¸ ì¤€ë¹„")
    print("-" * 80)
    
    train_dataset, val_dataset, test_dataset = create_train_val_test_datasets(
        NORMAL_HDF5_FILE, train_ratio=0.75, val_ratio=0.15, load_to_memory=LOAD_TO_MEMORY
    )
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
    test_loader = DataLoader(test_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=False, num_workers=0)
    
    model = CNNGRUAutoencoder(input_channels=2, sequence_length=2560, latent_dim=LATENT_DIM)
    
    if Path(MODEL_SAVE_PATH).exists():
        print(f"âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ: {MODEL_SAVE_PATH}")
        model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))
        model = model.to(DEVICE)
    else:
        print(f"âš ï¸  ê¸°ì¡´ ëª¨ë¸ ì—†ìŒ. ìƒˆë¡œ í•™ìŠµí•©ë‹ˆë‹¤...")
        model = model.to(DEVICE)
        train_autoencoder(model, train_loader, val_loader, EPOCHS, LEARNING_RATE, DEVICE, MODEL_SAVE_PATH)
    
    # ì„±ëŠ¥ í‰ê°€
    print("\n[2ë‹¨ê³„] ì„±ëŠ¥ í‰ê°€ (í…ŒìŠ¤íŠ¸ ë°ì´í„°ë§Œ ì‚¬ìš©)")
    print("-" * 80)
    print("âš ï¸  ì£¼ì˜: í•™ìŠµì— ì‚¬ìš©ë˜ì§€ ì•Šì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë§Œ í‰ê°€í•©ë‹ˆë‹¤!")
    
    evaluator = MedicalDevicePerformanceEvaluator(model, THRESHOLD_PERCENTILE)
    
    # í•™ìŠµ ë°ì´í„°ë¡œ ì„ê³„ê°’ ì„¤ì •
    print("\nğŸ¯ í•™ìŠµ ë°ì´í„°ë¡œ ì„ê³„ê°’ ì„¤ì •")
    evaluator.fit_threshold(train_loader, device=DEVICE)
    
    # ===== ë¹„ì •ìƒ ë°ì´í„° 5ê°œ í‰ê°€ =====
    print("\nğŸ“Š ë¹„ì •ìƒ ë°ì´í„°ì…‹ 5ê°œ í‰ê°€ ì‹œì‘")
    print("-" * 80)
    
    all_abnormal_errors = []
    all_abnormal_labels = []
    all_abnormal_preds = []
    
    for i, abnormal_file in enumerate(ABNORMAL_HDF5_FILES, 1):
        print(f"\n[ë¹„ì •ìƒ ë°ì´í„°ì…‹ {i}/{len(ABNORMAL_HDF5_FILES)}]")
        print(f"ğŸ“‚ íŒŒì¼: {Path(abnormal_file).name}")
        
        abnormal_dataset = load_dataset_auto(abnormal_file, load_to_memory=LOAD_TO_MEMORY)
        abnormal_loader = DataLoader(abnormal_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=False, num_workers=0)
        
        errors, labels, preds = evaluator.evaluate_dataset(
            abnormal_loader, true_label=1, dataset_name=f"ë¹„ì •ìƒ_{i}", device=DEVICE
        )
        
        all_abnormal_errors.append(errors)
        all_abnormal_labels.append(labels)
        all_abnormal_preds.append(preds)
        
        print(f"   âœ… í‰ê°€ ì™„ë£Œ: {len(errors)}ê°œ ìƒ˜í”Œ")
    
    # ëª¨ë“  ë¹„ì •ìƒ ë°ì´í„° ê²°í•©
    abnormal_errors = np.concatenate(all_abnormal_errors)
    abnormal_labels = np.concatenate(all_abnormal_labels)
    abnormal_preds = np.concatenate(all_abnormal_preds)
    
    print("\n" + "="*80)
    print(f"ğŸ“Š ë¹„ì •ìƒ ë°ì´í„° í†µí•© ê²°ê³¼")
    print(f"   ì´ ë¹„ì •ìƒ ìƒ˜í”Œ: {len(abnormal_errors):,}ê°œ")
    print(f"   ì˜ˆì¸¡ ê²°ê³¼: ì •ìƒ {np.sum(abnormal_preds==0):,}ê°œ, ë¹„ì •ìƒ {np.sum(abnormal_preds==1):,}ê°œ")
    print("="*80)
    
    # ===== ì •ìƒ ë°ì´í„°ë¥¼ ë¹„ì •ìƒ ë°ì´í„°ì™€ ê°™ì€ ì–‘ìœ¼ë¡œ ìƒ˜í”Œë§ =====
    print("\nğŸ“Š ì •ìƒ ë°ì´í„° í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€ (ë¹„ì •ìƒ ë°ì´í„°ì™€ ë™ì¼í•œ ìƒ˜í”Œ ìˆ˜)")
    print("-" * 80)
    
    total_abnormal_count = len(abnormal_errors)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ ë¹„ì •ìƒ ë°ì´í„° ê°œìˆ˜ë§Œí¼ë§Œ ìƒ˜í”Œë§
    if len(test_dataset) < total_abnormal_count:
        print(f"âš ï¸  ê²½ê³ : í…ŒìŠ¤íŠ¸ ë°ì´í„°({len(test_dataset)}ê°œ)ê°€ ë¹„ì •ìƒ ë°ì´í„°({total_abnormal_count}ê°œ)ë³´ë‹¤ ì ìŠµë‹ˆë‹¤.")
        print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        sampled_indices = list(range(len(test_dataset)))
    else:
        # ëœë¤ ìƒ˜í”Œë§ (ì¬í˜„ì„±ì„ ìœ„í•´ seed ì„¤ì •)
        np.random.seed(42)
        sampled_indices = np.random.choice(len(test_dataset), total_abnormal_count, replace=False).tolist()
        print(f"   ì •ìƒ í…ŒìŠ¤íŠ¸ ë°ì´í„° {len(test_dataset):,}ê°œ ì¤‘ {total_abnormal_count:,}ê°œ ìƒ˜í”Œë§")
    
    # ìƒ˜í”Œë§ëœ ì¸ë±ìŠ¤ë¡œ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ìƒì„±
    sampled_test_dataset = load_dataset_auto(NORMAL_HDF5_FILE, indices=[test_dataset.indices[i] for i in sampled_indices], load_to_memory=LOAD_TO_MEMORY)
    sampled_test_loader = DataLoader(sampled_test_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=False, num_workers=0)
    
    normal_errors, normal_labels, normal_preds = evaluator.evaluate_dataset(
        sampled_test_loader, true_label=0, dataset_name="ì •ìƒ (Test-Sampled)", device=DEVICE
    )
    
    print("\n" + "="*80)
    print(f"ğŸ“Š ìµœì¢… ë°ì´í„° ê· í˜• í™•ì¸")
    print(f"   ì •ìƒ ìƒ˜í”Œ: {len(normal_errors):,}ê°œ")
    print(f"   ë¹„ì •ìƒ ìƒ˜í”Œ: {len(abnormal_errors):,}ê°œ")
    print(f"   ìƒ˜í”Œ ë¹„ìœ¨: 1:1 (ê· í˜• ë§ì¶¤)")
    print("="*80)
    
    # ì „ì²´ ê²°í•©
    all_errors = np.concatenate([normal_errors, abnormal_errors])
    all_labels = np.concatenate([normal_labels, abnormal_labels])
    all_preds = np.concatenate([normal_preds, abnormal_preds])
    
    # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    metrics = evaluator.calculate_medical_metrics(all_labels, all_preds, all_errors)
    
    # ê²°ê³¼ ì¶œë ¥ ë° ì‹œê°í™”
    print_performance_report(metrics)
    plot_medical_performance_dashboard(metrics, normal_errors, abnormal_errors,
                                       save_path=f'{OUTPUT_DIR}/medical_performance_dashboard_5datasets.png')
    
    print("\nâœ… í‰ê°€ ì™„ë£Œ!")