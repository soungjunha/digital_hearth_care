import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import h5py
import matplotlib.pyplot as plt
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')


# ===== HDF5 ë°ì´í„°ì…‹ í´ë˜ìŠ¤ =====
class HDF5CardiacDataset(Dataset):
    """HDF5 í¬ë§· ECG, PPG ë°ì´í„°ì…‹ - ë©”ëª¨ë¦¬ íš¨ìœ¨ì """
    
    def __init__(self, hdf5_file, indices=None, load_to_memory=True):
        """
        Args:
            hdf5_file: HDF5 íŒŒì¼ ê²½ë¡œ
            indices: ì‚¬ìš©í•  ìƒ˜í”Œ ì¸ë±ìŠ¤ (Noneì´ë©´ ì „ì²´)
            load_to_memory: Trueë©´ ì „ì²´ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œ
        """
        self.hdf5_file = hdf5_file
        self.load_to_memory = load_to_memory
        
        # ë©”íƒ€ë°ì´í„° ì½ê¸°
        with h5py.File(hdf5_file, 'r') as hf:
            self.n_samples = hf.attrs['n_samples']
            self.sequence_length = hf.attrs['sequence_length']
            self.sampling_rate = hf.attrs.get('sampling_rate', 256)
            
            # ì¸ë±ìŠ¤ ì„¤ì •
            if indices is None:
                self.indices = list(range(self.n_samples))
            else:
                self.indices = indices
            
            # ì „ì²´ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œ
            if load_to_memory:
                self.ecg_data = hf['ecg'][:].astype(np.float32)
                self.ppg_data = hf['ppg'][:].astype(np.float32)
            else:
                self.ecg_data = None
                self.ppg_data = None
    
    def __len__(self):
        return len(self.indices)
    
    def __getitem__(self, idx):
        real_idx = self.indices[idx]
        
        if self.load_to_memory:
            ecg = self.ecg_data[real_idx]
            ppg = self.ppg_data[real_idx]
        else:
            with h5py.File(self.hdf5_file, 'r') as hf:
                ecg = hf['ecg'][real_idx]
                ppg = hf['ppg'][real_idx]
        
        # (2, sequence_length) í˜•íƒœë¡œ ìŠ¤íƒ
        sample = np.stack([ecg, ppg], axis=0)
        sample = torch.FloatTensor(sample)
        
        return sample, sample


def create_train_val_datasets(hdf5_file, train_ratio=0.75, load_to_memory=True):
    """í•™ìŠµ/ê²€ì¦ ë°ì´í„°ì…‹ ìë™ ë¶„í• """
    with h5py.File(hdf5_file, 'r') as hf:
        n_samples = hf.attrs['n_samples']
    
    indices = np.arange(n_samples)
    np.random.shuffle(indices)
    
    split_idx = int(n_samples * train_ratio)
    train_indices = indices[:split_idx].tolist()
    val_indices = indices[split_idx:].tolist()
    
    train_dataset = HDF5CardiacDataset(hdf5_file, indices=train_indices, load_to_memory=load_to_memory)
    val_dataset = HDF5CardiacDataset(hdf5_file, indices=val_indices, load_to_memory=load_to_memory)
    
    return train_dataset, val_dataset


# ===== CNN-GRU ì˜¤í† ì¸ì½”ë” ëª¨ë¸ =====
class CNNGRUAutoencoder(nn.Module):
    """CNN-GRU ê¸°ë°˜ ì˜¤í† ì¸ì½”ë”"""
    
    def __init__(self, input_channels=2, sequence_length=2560, latent_dim=64):
        super(CNNGRUAutoencoder, self).__init__()
        
        self.input_channels = input_channels
        self.sequence_length = sequence_length
        self.latent_dim = latent_dim
        
        # ===== ì¸ì½”ë” =====
        self.encoder_cnn = nn.Sequential(
            nn.Conv1d(input_channels, 32, kernel_size=7, stride=4, padding=3),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Conv1d(32, 64, kernel_size=5, stride=4, padding=2),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Conv1d(64, 128, kernel_size=5, stride=4, padding=2),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.2),
        )
        
        self.encoder_gru = nn.GRU(
            input_size=128,
            hidden_size=128,
            num_layers=2,
            batch_first=True,
            dropout=0.2,
            bidirectional=True
        )
        
        self.encoder_fc = nn.Linear(128 * 2, latent_dim)
        
        # ===== ë””ì½”ë” =====
        self.decoder_fc = nn.Linear(latent_dim, 128 * 40)
        
        self.decoder_gru = nn.GRU(
            input_size=128,
            hidden_size=128,
            num_layers=2,
            batch_first=True,
            dropout=0.2,
            bidirectional=False
        )
        
        self.decoder_cnn = nn.Sequential(
            nn.ConvTranspose1d(128, 64, kernel_size=5, stride=4, padding=2, output_padding=3),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.ConvTranspose1d(64, 32, kernel_size=5, stride=4, padding=2, output_padding=3),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.ConvTranspose1d(32, input_channels, kernel_size=7, stride=4, padding=3, output_padding=3),
        )
    
    def encode(self, x):
        x = self.encoder_cnn(x)
        x = x.permute(0, 2, 1)
        _, hidden = self.encoder_gru(x)
        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)
        z = self.encoder_fc(hidden)
        return z
    
    def decode(self, z):
        batch_size = z.size(0)
        x = self.decoder_fc(z)
        x = x.view(batch_size, 40, 128)
        x, _ = self.decoder_gru(x)
        x = x.permute(0, 2, 1)
        x = self.decoder_cnn(x)
        return x
    
    def forward(self, x):
        z = self.encode(x)
        reconstructed = self.decode(z)
        return reconstructed, z


# ===== ì´ìƒ íƒì§€ê¸° =====
class AnomalyDetector:
    """ì¬êµ¬ì„± ì˜¤ì°¨ ê¸°ë°˜ ì´ìƒ íƒì§€"""
    
    def __init__(self, model, threshold_percentile=95):
        self.model = model
        self.threshold_percentile = threshold_percentile
        self.threshold = None
        self.reconstruction_errors = []
    
    def calculate_reconstruction_error(self, original, reconstructed):
        mse = torch.mean((original - reconstructed) ** 2, dim=(1, 2))
        return mse
    
    def fit_threshold(self, dataloader, device='cpu'):
        self.model.eval()
        errors = []
        
        with torch.no_grad():
            for data, _ in dataloader:
                data = data.to(device)
                reconstructed, _ = self.model(data)
                error = self.calculate_reconstruction_error(data, reconstructed)
                errors.extend(error.cpu().numpy())
        
        self.reconstruction_errors = errors
        self.threshold = np.percentile(errors, self.threshold_percentile)
        print(f"ì´ìƒ íƒì§€ ì„ê³„ê°’ ì„¤ì •: {self.threshold:.6f} ({self.threshold_percentile}th percentile)")
        
        return self.threshold
    
    def detect(self, data, device='cpu'):
        self.model.eval()
        
        with torch.no_grad():
            data = data.to(device)
            reconstructed, latent = self.model(data)
            error = self.calculate_reconstruction_error(data, reconstructed)
        
        is_anomaly = error > self.threshold
        
        return {
            'is_anomaly': is_anomaly.cpu().numpy(),
            'error': error.cpu().numpy(),
            'threshold': self.threshold,
            'reconstructed': reconstructed.cpu().numpy(),
            'latent': latent.cpu().numpy()
        }
    
    def get_warning_level(self, error):
        if error < self.threshold:
            return "ì •ìƒ", 0
        elif error < self.threshold * 1.5:
            return "ê²½ë¯¸í•œ ì´ìƒ", 1
        elif error < self.threshold * 2.0:
            return "ì£¼ì˜", 2
        else:
            return "ì‹¬ê°í•œ ì´ìƒ", 3


# ===== í•™ìŠµ í•¨ìˆ˜ =====
def train_autoencoder(model, train_loader, val_loader, epochs=100, lr=0.001, device='cpu', model_save_path='best_model.pth'):
    """ì˜¤í† ì¸ì½”ë” í•™ìŠµ"""
    from tqdm import tqdm
    
    model = model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)
    criterion = nn.MSELoss()
    
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    
    print("âœ… ì²« ë²ˆì§¸ ë°°ì¹˜ ë¡œë”© ì™„ë£Œ! í•™ìŠµ ì§„í–‰ ì¤‘...\n")
    
    for epoch in range(epochs):
        # í•™ìŠµ
        model.train()
        train_loss = 0.0
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]', ncols=100)
        for batch_idx, (data, _) in enumerate(pbar):
            data = data.to(device)
            
            optimizer.zero_grad()
            reconstructed, _ = model(data)
            loss = criterion(reconstructed, data)
            
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            
            # ì‹¤ì‹œê°„ ì†ì‹¤ í‘œì‹œ
            pbar.set_postfix({'loss': f'{loss.item():.6f}'})
        
        train_loss /= len(train_loader)
        train_losses.append(train_loss)
        
        # ê²€ì¦
        model.eval()
        val_loss = 0.0
        
        with torch.no_grad():
            pbar_val = tqdm(val_loader, desc=f'Epoch {epoch+1}/{epochs} [Val]', ncols=100, leave=False)
            for data, _ in pbar_val:
                data = data.to(device)
                reconstructed, _ = model(data)
                loss = criterion(reconstructed, data)
                val_loss += loss.item()
                
                pbar_val.set_postfix({'loss': f'{loss.item():.6f}'})
        
        val_loss /= len(val_loader)
        val_losses.append(val_loss)
        
        scheduler.step(val_loss)
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), model_save_path)
            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f} â­ (Best)')
        else:
            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')
    
    return train_losses, val_losses


# ===== ì‹œê°í™” í•¨ìˆ˜ =====
def visualize_reconstruction(original, reconstructed, sample_idx=0, save_path=None):
    """Compare original and reconstructed signals"""
    fig, axes = plt.subplots(2, 1, figsize=(15, 8))
    
    axes[0].plot(original[sample_idx, 0, :], label='Original ECG', alpha=0.7, linewidth=1.5)
    axes[0].plot(reconstructed[sample_idx, 0, :], label='Reconstructed ECG', alpha=0.7, linewidth=1.5)
    axes[0].set_title('ECG Signal Reconstruction', fontsize=14, fontweight='bold')
    axes[0].set_xlabel('Sample (256Hz)', fontsize=12)
    axes[0].set_ylabel('Amplitude', fontsize=12)
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    axes[1].plot(original[sample_idx, 1, :], label='Original PPG', alpha=0.7, linewidth=1.5)
    axes[1].plot(reconstructed[sample_idx, 1, :], label='Reconstructed PPG', alpha=0.7, linewidth=1.5)
    axes[1].set_title('PPG Signal Reconstruction', fontsize=14, fontweight='bold')
    axes[1].set_xlabel('Sample (256Hz)', fontsize=12)
    axes[1].set_ylabel('Amplitude', fontsize=12)
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()


def plot_training_curves(train_losses, val_losses, save_path=None):
    """Plot training curves"""
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Train Loss', linewidth=2)
    plt.plot(val_losses, label='Validation Loss', linewidth=2)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss (MSE)', fontsize=12)
    plt.title('Training Curves', fontsize=14, fontweight='bold')
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()


def plot_anomaly_distribution(errors, threshold, save_path=None):
    """Plot anomaly score distribution"""
    plt.figure(figsize=(12, 6))
    
    plt.hist(errors, bins=50, alpha=0.7, edgecolor='black')
    plt.axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {threshold:.6f}')
    plt.xlabel('Reconstruction Error (MSE)', fontsize=12)
    plt.ylabel('Frequency', fontsize=12)
    plt.title('Reconstruction Error Distribution', fontsize=14, fontweight='bold')
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()


# ===== ë©”ì¸ ì‹¤í–‰ =====
if __name__ == "__main__":
    # ==========================================
    # ğŸ›ï¸ í•™ìŠµ ì„¤ì •
    # ==========================================
    
    # ğŸ“ íŒŒì¼ ê²½ë¡œ
    HDF5_FILE = r'C:\Users\jerom\Downloads\model\dataset.h5'  # ì…ë ¥ HDF5 íŒŒì¼
    MODEL_SAVE_PATH = r'C:\Users\jerom\Downloads\model\model_test.pth'  # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
    OUTPUT_DIR = r'C:\Users\jerom\Downloads\model\plots'  # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    
    # ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    BATCH_SIZE = 128
    EPOCHS = 100
    LEARNING_RATE = 0.001
    LATENT_DIM = 128
    TRAIN_RATIO = 0.8
    LOAD_TO_MEMORY = True
    THRESHOLD_PERCENTILE = 95
    
    # ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤ ì„¤ì •
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ==========================================
    
    print("="*70)
    print("ğŸ§  ì‹¬ì¥ì§ˆí™˜ ì´ìƒì§•í›„ íƒì§€ ëª¨ë¸ í•™ìŠµ")
    print("="*70)
    print(f"ğŸ“ HDF5 íŒŒì¼: {HDF5_FILE}")
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {MODEL_SAVE_PATH}")
    print(f"ğŸ–¥ï¸  ë””ë°”ì´ìŠ¤: {DEVICE}")
    print(f"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}")
    print(f"ğŸ”„ ì—í¬í¬: {EPOCHS}")
    print(f"ğŸ“ˆ í•™ìŠµë¥ : {LEARNING_RATE}")
    print(f"ğŸ¯ ì ì¬ ì°¨ì›: {LATENT_DIM}")
    print(f"ğŸ“‰ í•™ìŠµ ë¹„ìœ¨: {TRAIN_RATIO*100:.0f}%")
    print(f"ğŸš¨ ì„ê³„ê°’ ë°±ë¶„ìœ„: {THRESHOLD_PERCENTILE}")
    print("="*70)
    
    # ===== 1. HDF5 íŒŒì¼ ë¡œë“œ =====
    print("\nHDF5 íŒŒì¼ ë¡œë“œ ì¤‘...")
    hdf5_file = HDF5_FILE
    
    with h5py.File(hdf5_file, 'r') as hf:
        print(f"ì „ì²´ ìƒ˜í”Œ ìˆ˜: {hf.attrs['n_samples']}")
        print(f"ì‹œí€€ìŠ¤ ê¸¸ì´: {hf.attrs['sequence_length']}")
        print(f"ìƒ˜í”Œë§ë¥ : {hf.attrs['sampling_rate']} Hz")
    
    # ===== 2. ë°ì´í„°ì…‹ ìƒì„± =====
    train_dataset, val_dataset = create_train_val_datasets(
        hdf5_file, 
        train_ratio=TRAIN_RATIO,
        load_to_memory=LOAD_TO_MEMORY
    )
    
    # DataLoader ì„¤ì • (ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ìµœì í™”)
    # Windowsì—ì„œëŠ” num_workers=0 í•„ìˆ˜!
    import platform
    is_windows = platform.system() == 'Windows'
    num_workers = 0 if is_windows else (4 if DEVICE.type == 'cuda' else 2)
    
    print(f"\nDataLoader ì„¤ì •:")
    print(f"  - num_workers: {num_workers} ({'Windows í˜¸í™˜ ëª¨ë“œ' if is_windows else 'Linux/Mac ëª¨ë“œ'})")
    print(f"  - pin_memory: {DEVICE.type == 'cuda'}")
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True if DEVICE.type == 'cuda' else False,
        prefetch_factor=2 if num_workers > 0 else None,
        persistent_workers=True if num_workers > 0 else False
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True if DEVICE.type == 'cuda' else False,
        prefetch_factor=2 if num_workers > 0 else None,
        persistent_workers=True if num_workers > 0 else False
    )
    
    print(f"\ní•™ìŠµ ë°ì´í„°: {len(train_dataset)}ê°œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(val_dataset)}ê°œ")
    
    # ===== 3. ëª¨ë¸ ìƒì„± ë° í•™ìŠµ =====
    print("\nëª¨ë¸ ìƒì„±...")
    model = CNNGRUAutoencoder(
        input_channels=2,
        sequence_length=2560,
        latent_dim=LATENT_DIM
    )
    
    print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    print("\ní•™ìŠµ ì‹œì‘...")
    print("â³ ì²« ë²ˆì§¸ ë°°ì¹˜ ë¡œë”© ì¤‘... (10-30ì´ˆ ì†Œìš”)")
    
    train_losses, val_losses = train_autoencoder(
        model, train_loader, val_loader,
        epochs=EPOCHS, 
        lr=LEARNING_RATE, 
        device=DEVICE,
        model_save_path=MODEL_SAVE_PATH
    )
    
    # í•™ìŠµ ê³¡ì„  ì‹œê°í™”
    plot_training_curves(
        train_losses, val_losses, 
        f'{OUTPUT_DIR}/training_curves_hdf5.png'
    )
    
    # ===== 4. ì´ìƒ íƒì§€ ì„¤ì • =====
    print("\nì´ìƒ íƒì§€ ì„ê³„ê°’ ì„¤ì • ì¤‘...")
    detector = AnomalyDetector(model, threshold_percentile=THRESHOLD_PERCENTILE)
    detector.fit_threshold(train_loader, device=DEVICE)
    
    # ì´ìƒ ì ìˆ˜ ë¶„í¬ ì‹œê°í™”
    plot_anomaly_distribution(
        detector.reconstruction_errors,
        detector.threshold,
        f'{OUTPUT_DIR}/anomaly_distribution_hdf5.png'
    )
    
    # ===== 5. ì „ì²´ ë°ì´í„° í‰ê°€ =====
    print("\nì „ì²´ ë°ì´í„° í‰ê°€ ì¤‘...")
    
    # ì „ì²´ ë°ì´í„° ë¡œë“œ
    full_dataset = HDF5CardiacDataset(hdf5_file, load_to_memory=True)
    full_loader = DataLoader(full_dataset, batch_size=len(full_dataset), shuffle=False)
    
    for data, _ in full_loader:
        result = detector.detect(data, device=DEVICE)
        
        print("\n[ì „ì²´ ë°ì´í„° ì´ìƒ íƒì§€ ê²°ê³¼]")
        print("-" * 60)
        
        with h5py.File(hdf5_file, 'r') as hf:
            filenames = [name.decode() if isinstance(name, bytes) else name 
                        for name in hf['filenames'][:]]
        
        for i, filename in enumerate(filenames):
            warning_level, severity = detector.get_warning_level(result['error'][i])
            anomaly_status = "ğŸš¨ ì´ìƒ" if result['is_anomaly'][i] else "âœ“ ì •ìƒ"
            print(f"{filename:20s} | {anomaly_status:8s} | "
                  f"ì˜¤ì°¨: {result['error'][i]:.6f} | {warning_level}")
        print("-" * 60)
        
        # ì²« ë²ˆì§¸ ìƒ˜í”Œ ì‹œê°í™”
        visualize_reconstruction(
            data.cpu().numpy(),
            result['reconstructed'],
            sample_idx=0,
            save_path=f'{OUTPUT_DIR}/reconstruction_hdf5.png'
        )
        break
    
    print("\n" + "="*70)
    print("âœ… HDF5 ê¸°ë°˜ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì™„ë£Œ!")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}")
    print(f"  - ëª¨ë¸: {MODEL_SAVE_PATH}")
    print(f"  - í•™ìŠµ ê³¡ì„ : {OUTPUT_DIR}/training_curves_hdf5.png")
    print(f"  - ì´ìƒ ë¶„í¬: {OUTPUT_DIR}/anomaly_distribution_hdf5.png")
    print(f"  - ì¬êµ¬ì„± ì˜ˆì‹œ: {OUTPUT_DIR}/reconstruction_hdf5.png")
    print("="*70)